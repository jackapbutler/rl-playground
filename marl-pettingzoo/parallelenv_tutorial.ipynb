{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium.spaces import Discrete\n",
    "\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.utils import parallel_to_aec, wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "NONE = 3\n",
    "\n",
    "MOVES = [\"ROCK\", \"PAPER\", \"SCISSORS\", \"None\"]\n",
    "\n",
    "NUM_ITERS = 100\n",
    "\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0, 0),\n",
    "    (ROCK, PAPER): (-1, 1),\n",
    "    (ROCK, SCISSORS): (1, -1),\n",
    "    (PAPER, ROCK): (1, -1),\n",
    "    (PAPER, PAPER): (0, 0),\n",
    "    (PAPER, SCISSORS): (-1, 1),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "    (SCISSORS, SCISSORS): (0, 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parallel_env(ParallelEnv):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
    "\n",
    "    def __init__(self, num_players, render_mode=None):\n",
    "        \"\"\"\n",
    "        The init method takes in environment arguments and should define the following attributes:\n",
    "        - possible_agents\n",
    "        - action_spaces\n",
    "        - observation_spaces\n",
    "        These attributes should not be changed after initialization.\n",
    "        \"\"\"\n",
    "        self.possible_agents = [\"player_\" + str(r) for r in range(num_players)]\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    # this cache ensures that same space object is returned for the same agent\n",
    "    # allows action space seeding to work as expected\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return Discrete(4)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the environment. In human mode, it can print to terminal, open\n",
    "        up a graphical window, or open up some other display that a human can see and understand.\n",
    "        \"\"\"\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if len(self.agents) == 2:\n",
    "            string = \"Current state: Agent1: {} , Agent2: {}\".format(\n",
    "                MOVES[self.state[self.agents[0]]], MOVES[self.state[self.agents[1]]]\n",
    "            )\n",
    "        else:\n",
    "            string = \"Game over\"\n",
    "        print(string)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections\n",
    "        or any other environment data which should not be kept around after the\n",
    "        user is no longer using the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, return_info=False, options=None):\n",
    "        \"\"\"\n",
    "        Reset needs to initialize the `agents` attribute and must set up the\n",
    "        environment so that render(), and step() can be called without issues.\n",
    "        Here it initializes the `num_moves` variable which counts the number of\n",
    "        hands that are played.\n",
    "        Returns the observations for each agent\n",
    "        \"\"\"\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.num_moves = 0\n",
    "        observations = {agent: NONE for agent in self.agents}\n",
    "\n",
    "        if not return_info:\n",
    "            return observations\n",
    "        else:\n",
    "            infos = {agent: {} for agent in self.agents}\n",
    "            return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        step(action) takes in an action for each agent and should return the\n",
    "        - observations\n",
    "        - rewards\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        dicts where each dict looks like {agent_1: item_1, agent_2: item_2}\n",
    "        \"\"\"\n",
    "        # If a user passes in actions with no agents, then just return empty observations, etc.\n",
    "        if not actions:\n",
    "            self.agents = []\n",
    "            return {}, {}, {}, {}, {}\n",
    "\n",
    "        # rewards for all agents are placed in the rewards dictionary to be returned\n",
    "        rewards = {}\n",
    "        rewards[self.agents[0]], rewards[self.agents[1]] = REWARD_MAP[\n",
    "            (actions[self.agents[0]], actions[self.agents[1]])\n",
    "        ]\n",
    "\n",
    "        terminations = {agent: False for agent in self.agents}\n",
    "\n",
    "        self.num_moves += 1\n",
    "        env_truncation = self.num_moves >= NUM_ITERS\n",
    "        truncations = {agent: env_truncation for agent in self.agents}\n",
    "\n",
    "        # current observation is just the other player's most recent action\n",
    "        observations = {\n",
    "            self.agents[i]: int(actions[self.agents[1 - i]])\n",
    "            for i in range(len(self.agents))\n",
    "        }\n",
    "\n",
    "        # typically there won't be any information in the infos, but there must\n",
    "        # still be an entry for each agent\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "\n",
    "        if env_truncation:\n",
    "            self.agents = []\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return observations, rewards, terminations, truncations, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_env(num_players, render_mode=None):\n",
    "    \"\"\"\n",
    "    To support the AEC API, the raw_env() function just uses the from_parallel\n",
    "    function to convert from a ParallelEnv to an AEC env\n",
    "    \"\"\"\n",
    "    env = parallel_env(num_players, render_mode=render_mode)\n",
    "    env = parallel_to_aec(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env(num_players, render_mode=None):\n",
    "    \"\"\"\n",
    "    The env function often wraps the environment in wrappers by default.\n",
    "    You can find full documentation for these methods\n",
    "    elsewhere in the developer documentation.\n",
    "    \"\"\"\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(num_players, render_mode=internal_render_mode)\n",
    "    # This wrapper is only for environments which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    # this wrapper helps error handling for discrete action spaces\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    # Provides a wide vareity of helpful user errors\n",
    "    # Strongly recommended\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = env(2, 'human')\n",
    "\n",
    "example.reset(return_info=True)\n",
    "observation = example.observe('player_0')\n",
    "\n",
    "observation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
